{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lemmatization_Stemming.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPoYWhPsH07wMlWUqqGBDdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkarsha1a/NLP/blob/main/Lemmatization_Stemming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Stemming and Lemmatization in Python NLTK?\n",
        "Stemming and Lemmatization in Python NLTK are text **normalization techniques** for Natural Language Processing. These techniques are widely used for text preprocessing. \n",
        "\n",
        "### The difference between stemming and lemmatization is that \n",
        "\n",
        "stemming it is faster as it cuts words without knowing the context. \n",
        "\n",
        "lemmatization provides better results by performing an analysis that depends on the word's part-of-speech and producing real, dictionary words. As a result, lemmatization is harder to implement and slower compared to stemming.\n"
      ],
      "metadata": {
        "id": "fwRlIeRcJzW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Example** Word - 'Studying'\n",
        "\n",
        "Lemmatize -- 'Study'. \n",
        "Stem -- 'Studi' and this is erroneous."
      ],
      "metadata": {
        "id": "nxAPNYwKK7Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import \tWordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "HVm89RpPL0DF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_data = \"studies studying caring very history historical\""
      ],
      "metadata": {
        "id": "nmVQAdXiKffD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First Word tokenization\n",
        "nltk_tokens = nltk.word_tokenize(word_data)"
      ],
      "metadata": {
        "id": "vteL97bRM5xU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "#Next find the roots of the word\n",
        "for w in nltk_tokens:\n",
        "  print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w))) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWMGFEBvL5nH",
        "outputId": "bcc7ae67-2e88-452f-cbe6-470c118d4557"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming for studies is studi\n",
            "Stemming for studying is studi\n",
            "Stemming for caring is care\n",
            "Stemming for very is veri\n",
            "Stemming for history is histori\n",
            "Stemming for historical is histor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "for w in nltk_tokens:\n",
        "\tprint(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1tDQ55CMr5U",
        "outputId": "ae0857f1-0d9f-4d2f-bafe-6fda7c9d56aa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for studies is study\n",
            "Lemma for studying is studying\n",
            "Lemma for caring is caring\n",
            "Lemma for very is very\n",
            "Lemma for history is history\n",
            "Lemma for historical is historical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code we are removing stop words.\n",
        "\n",
        "Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document. Generally, the most common words used in a text are “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” et"
      ],
      "metadata": {
        "id": "0Q-nr_-qP92e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(stopwords.words('english')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQMIDzPGQvEZ",
        "outputId": "32add4e5-a863-4fff-8647-eec28dfc370a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'he', 'am', 've', \"shouldn't\", 'doing', 'o', 'mustn', 'mightn', \"you'll\", 'these', 'the', 'more', 'an', 'couldn', 'doesn', 'then', 'you', 'during', 'myself', \"don't\", 'as', 'do', \"wouldn't\", 'yourselves', 'from', 'weren', 'yourself', 'herself', 'itself', 'have', 'they', 'why', 'her', 'ain', 'ours', 'just', 'ma', 'she', 'be', 'through', 'was', \"weren't\", 'won', \"should've\", 'off', \"mightn't\", \"that'll\", 'once', 'can', 'but', 'to', 'our', 'them', \"doesn't\", 'above', 'when', 'how', 'other', 'his', 'who', 'in', \"didn't\", 'again', 'll', \"you've\", \"you're\", 'its', \"shan't\", 'by', 'or', 'don', 'had', 'of', 'shan', 's', 'm', 'those', 'their', 'haven', 'does', 'being', 'while', 'up', 'under', \"won't\", 'some', 'are', \"isn't\", 'until', 'wasn', 'where', 'which', 'nor', 't', 'not', 'y', 'been', 'few', 'isn', 'out', 'here', 'hasn', \"it's\", 'about', 'your', 'if', 'no', 'because', 'before', 'so', 'there', 'i', 'too', \"mustn't\", 'same', 'should', 'needn', 'themselves', 'after', 'himself', 'd', 'aren', 'into', 'very', 'ourselves', 'down', 'only', \"wasn't\", 'at', 'hers', 'we', 'below', 'yours', 'own', 'now', 'all', 'over', 'theirs', 'for', 'him', 'most', \"needn't\", 'shouldn', 'this', \"couldn't\", 'on', \"hadn't\", \"hasn't\", 'between', 'having', 'were', 'against', 'wouldn', 'is', 're', 'each', 'didn', 'hadn', 'did', 'what', 'both', 'my', 'will', 'with', 'further', \"you'd\", 'and', 'than', 'it', \"haven't\", 'that', \"she's\", 'any', \"aren't\", 'has', 'me', 'a', 'such', 'whom'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_sent = \"\"\"Data science is an interdisciplinary field that uses scientific methods, processes, \n",
        "                  algorithms and systems to extract knowledge and insights from data in various forms, both \n",
        "                  structured and unstructured,[1][2] similar to data mining.\"\"\"\n",
        " \n",
        "stop_words = set(stopwords.words('english'))\n",
        " \n",
        "word_tokens = word_tokenize(example_sent)\n",
        "print(word_tokens)\n",
        "\n",
        "filtered_sentence = []\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "print(filtered_sentence)\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "for w in filtered_sentence:\n",
        "\tprint(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_WmEZLOZwxW",
        "outputId": "5c25bf84-c6a4-49c6-f3d0-32285e96a56f"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Data', 'science', 'is', 'an', 'interdisciplinary', 'field', 'that', 'uses', 'scientific', 'methods', ',', 'processes', ',', 'algorithms', 'and', 'systems', 'to', 'extract', 'knowledge', 'and', 'insights', 'from', 'data', 'in', 'various', 'forms', ',', 'both', 'structured', 'and', 'unstructured', ',', '[', '1', ']', '[', '2', ']', 'similar', 'to', 'data', 'mining', '.']\n",
            "['Data', 'science', 'interdisciplinary', 'field', 'uses', 'scientific', 'methods', ',', 'processes', ',', 'algorithms', 'systems', 'extract', 'knowledge', 'insights', 'data', 'various', 'forms', ',', 'structured', 'unstructured', ',', '[', '1', ']', '[', '2', ']', 'similar', 'data', 'mining', '.']\n",
            "Lemma for Data is Data\n",
            "Lemma for science is science\n",
            "Lemma for interdisciplinary is interdisciplinary\n",
            "Lemma for field is field\n",
            "Lemma for uses is us\n",
            "Lemma for scientific is scientific\n",
            "Lemma for methods is method\n",
            "Lemma for , is ,\n",
            "Lemma for processes is process\n",
            "Lemma for , is ,\n",
            "Lemma for algorithms is algorithm\n",
            "Lemma for systems is system\n",
            "Lemma for extract is extract\n",
            "Lemma for knowledge is knowledge\n",
            "Lemma for insights is insight\n",
            "Lemma for data is data\n",
            "Lemma for various is various\n",
            "Lemma for forms is form\n",
            "Lemma for , is ,\n",
            "Lemma for structured is structured\n",
            "Lemma for unstructured is unstructured\n",
            "Lemma for , is ,\n",
            "Lemma for [ is [\n",
            "Lemma for 1 is 1\n",
            "Lemma for ] is ]\n",
            "Lemma for [ is [\n",
            "Lemma for 2 is 2\n",
            "Lemma for ] is ]\n",
            "Lemma for similar is similar\n",
            "Lemma for data is data\n",
            "Lemma for mining is mining\n",
            "Lemma for . is .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One major difference with stemming is that lemmatize takes a part of speech parameter, “pos” If not supplied, the default is “noun.”\n",
        "Below is the implementation of lemmatization words using NLTK:"
      ],
      "metadata": {
        "id": "k6O_OrS9a0bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        " \n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        " \n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdHfRBJDTMiZ",
        "outputId": "4d5dbb68-b89a-407e-ad79-b31e7c398728"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ]
        }
      ]
    }
  ]
}